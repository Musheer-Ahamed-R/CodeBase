CockroachDB debuted as the open source Distributed SQL database that made it possible to build massive, reliable cloud applications without giving up SQL

High Availability. No Single point of failure  // check how master slave is done or can be done

Since it also supports Postgress Wire protocol for driver and ORM support. We can connect applications to it
just like as we would in postgress database

However, many still rely on monolithic relational databases for application state ???
These databases are stalling growth as applications ultimately depend on a hard-to-update,
single point of failure that can’t take advantage of the nearly unlimited, on-demand resources available in the cloud.



Inverted indices to speed up queries on giant volumes of data without having to map out the types of requests you want to process ahead of time.
// Learn more

Many cloud-native relational databases can scale to support certain types of queries by adding special servers(e.g using Kubernets)
that exclusively handle reads, while restricting all writes to a single, master node; if you ever need to scale writes,
you must bring down your database while scaling up the master instance to a more powerful machine.
// Can we not do it without bringing down the database


In cockroach db, developers just add more nodes to the cluster
CockroachDB nodes self-organize to balance reads and writes while simultaneously 
Because CockroachDB enables no-downtime scaling out to support huge increases in read and write traffic
// How does it self organtizes if we add more nodes - This is a property of Cockroach DB

*****
The Write can be done by any machine in the cluster, unlike other database services where it has to be done in a single machine all the time,
but data can be read from any machine.
// CHECK THIS


It automatically replicates, rebalances, and repairs itself

Offering Highest Isolation levels specified in the SQL standard
// Check more ??		// Serializable


working with multi-regional data

However, as the breadth of the customer base grows, maybe from a US-only audience to a global one, operators experience a growing chorus of complaints about remote customers’ long wait times, as requests must travel from the customer all the way to their data, which may be an ocean away

We’ve also added Geo-partitioning, an incredibly powerful feature that lets you control where your data lives at the individual record level. With geo-partitioning, you create policies that, for instance, pin a customer’s data in the closest datacenter to keep end-to-end latencies low.


Doubt:
Since distributed, what is the IP Address we use for connecting to Cockroach db.(i.e There are mutiple machines with different IPS)


Working of Cockroach DB

-> Takes all data. Sorts it and breaks it up into 64 Mega Byte chunks called ranges and then replicates each one of those ranges three 
times by default and spreads those replicas.

-> If we add clusters across regions, This can have huge implications on our latency(lets assume only one replica in the US), because of this
if we are going to do a write we have to go to alteast one other nodes in the other region(Huge round trip cost)
Also, if all the data(along with its replica) is going to be availabe in a one region, then, read will also have the huge round trip cost 

-> Internally CockroachDB is a Key value store.(i.e RocksDB)
		The Node that the App/Client connects to is called the Gateway Node
		The Layer that out App connects to is the SQL layer. This layer creates Logical and phycical plans which it sends to the Transaction and Distribution layer

		For Writes - The Distribution layer maps the SQL statements into Key-Value pairs in the form of 64 MB Chunks of data called Ranges

-> To ensure consistency, CockroachDB uses a consensus protocol, that requires a "Quorum" on any changes to a range, 
	 before those changes are Commited. Consensus Protocol used in called Raft.

-> Each range is replicated to alteast 3 nodes.

-> The number of failures that can be tolerated = (replication factor - 1) / 2. (i.e if the replication factor is 3, 1 node failure can be tolerated.
	 In that case the ranges are under-replicated but still available.

-> In case of 3 replication factor and two nodes failed. Then the Quorom will not be met and the range becomes unavailable.

Solution
    - Shard the database, create a slot/silo for US and silo for paris customers, but in this case we lose the ability to query across 
      silos. (Cannot access data from US in Paris webserver)
    - Use GEO partitioning. Create a partion based on some column also say which countries lives in which partition(i.e grouping few countries together in one partition)
      Configure the partition to a specific node range (e.g US region)


Node goes Down:
	-> Traditional RDBMS goes down, along with all dependent applications. 	
	-> NOSql database risks inconsistency
	-> CockroachDB offers high availability and consistency.
	// CHECK THIS

Performance loggging
	-> Entire quesies are logged and can be viewed in the console.
	https://youtu.be/NLnyUoHvDG8?list=PL_QaflmEF2e-zNgFcD0AduMSUtn9iViSN

Query with explain ???
What happens when a GET is made before the data is replicated ??
Cockroach DB admin
Problem:  If you're connected a node that goes down, the client will receive an error message and can simply restart the transaction.
					because of CockroachDB's consistency guarantees, you won't have inconsistent/partially updated data. And, because it is
					highly available, you'll be able to successfully restart the transaction unless a majority of your replicas go down.

Alternatives:
	- Google Cloud Spanner
	- not sure of JSON support
	- Dev time in understanding and structuring whats best for Google Spanner to get best results(Choosing indexes, referential integrity, PK values etc..)
	- Spanner isn’t MySQL compliant, nor even ANSI SQL compliant, and doesn’t plan to be.(Migrating)		// CHECK
	- Costs a Lot